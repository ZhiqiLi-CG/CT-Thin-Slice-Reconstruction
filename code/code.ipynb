{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled6.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMvh5s5uPhC+eYZURRhBYaj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yL_vLMD3essJ"},"source":["近日工作流程\n","\n","1.在不同的feature map大小下测试ESPCN模型\n","\n","2.改造VESPCN模型\n","\n","3.熟悉Zero shot模型\n","\n","4.改进data loader"]},{"cell_type":"markdown","source":["把数据加载器加进来"],"metadata":{"id":"W3gdXm0TDWJb"}},{"cell_type":"code","metadata":{"id":"HbGQ-uVryyQf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639041299842,"user_tz":-480,"elapsed":24633,"user":{"displayName":"黎治圻","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15483283087615354405"}},"outputId":"09e8cf69-25de-443d-a50a-ece3072f5748"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /\n","%cd content/drive/MyDrive/Colab Notebooks/CT_project/"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/\n","/content/drive/MyDrive/Colab Notebooks/CT_project\n"]}]},{"cell_type":"code","source":["#在这里需要开始运行处理程序\n","#这一步处理程序中，可能还是需要分割，但是需要注意的是，分割的图片需要同时运用于两个子模型上\n","import cv2 \n","import numpy as np\n","import os,sys\n","import shutil\n","from __future__ import absolute_import\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.autograd import Variable\n","import torchvision\n","from torch.nn import init\n","import functools\n","import shutil\n","from torch.utils.data import Dataset,DataLoader,TensorDataset\n","all_depth=256\n","all_cnt=0\n","train_batch=64\n","split_scale_H=4\n","split_scale_W=8\n","find_width=None\n","find_height=None\n","frame_num=3\n","scale=4\n","#顺序2：一个文件夹下的图片是一个物体的多个截面，读取一个文件夹下的图片\n","#最开始的图片的排列顺序是width*heigth\n","def read_path(file_pathname):\n","    #遍历该目录下的所有图片文件\n","    img_list=[]\n","    files=os.listdir(file_pathname)\n","    tem_files=[]\n","    for o in files:\n","        if(not o[0]=='.'):\n","            tem_files.append(o)\n","    files=tem_files\n","    files.sort(key= lambda x:int(x[-9:-4]))\n","    cnt=0\n","    for filename in files:\n","        if(cnt==all_depth):\n","          break\n","        img = cv2.imread(file_pathname+'/'+filename)\n","        img_list.append(img[:,:,0])\n","        cnt=cnt+1\n","    print((\"read the model\"+file_pathname+\",there are files:\",len(img_list)))\n","    return img_list\n","#顺序3：将read_path得到的若干张图片进行纵向切割\n","#图片数量只去all_depth张\n","#局部变量：width图片的宽度，height图片的长度，depth是图片的数量（高度）\n","#         new_img_list1的切割方向是生成depth*width，而new_img_list2的切割方向是生成depth*height\n","#全局变量：find_height,find_width传递width和height，scale是对其降采样的倍数。\n","#注意：再这里还没有产生video\n","def downsampling_mean(img_list,down_scale):\n","    new_img_list=[]\n","    for i in range(0,len(img_list)):\n","        img,k=img_list[i].copy(),0\n","        for j in range(0,img_list[i].shape[0],down_scale):\n","            new_line=np.sum(img_list[i][j:j+down_scale,:],axis=0)/down_scale\n","            img[k,:]=new_line.copy()\n","            k=k+1\n","        new_img_list.append(img[0:k,:].copy())\n","    return new_img_list\n","def generate_image(dir_path):\n","    img_list=read_path(dir_path)\n","    if(len(img_list)<all_depth):\n","        print(\"invalid image list for too few images\")\n","        return [],[]\n","    width,height=img_list[0].shape[0],img_list[0].shape[1]\n","    global find_height,find_width;\n","    find_height,find_width=height,width\n","    depth=all_depth #len(img_list)#这个地方貌似有点问题?\n","    new_img_list1,new_img_list2=[],[]\n","    for i in range(0,height):\n","        img = np.zeros((depth,width), np.uint8)\n","        new_img_list1.append(img)\n","    for i in range(0,width):\n","        img = np.zeros((depth,height), np.uint8)\n","        new_img_list2.append(img)\n","    for j in range(0,depth):\n","        for i in range(0,height):\n","            img=img_list[j]\n","            new_img_list1[i][j,0:width]=img[0:width,i].reshape(1,width).copy()\n","    for j in range(0,depth):\n","        for i in range(0,width):\n","            img=img_list[j]\n","            new_img_list2[i][j,0:height]=img[i,0:height].copy()\n","    #这里做down-sampling，应该需要通过函数来做\n","    new_img_list3,new_img_list4=downsampling_mean(new_img_list1,scale),downsampling_mean(new_img_list2,scale)\n","    return (new_img_list1,new_img_list2,new_img_list3,new_img_list4)\n","#  \"\"\"\n","#  在这一个函数中,需要将new_img_list进行裁剪,裁剪比例为split_scale_H与split_scale_W\n","#  可以发现裁剪过后的结果它的顺序是行排除的.，先把一行排满了，然后再来排一列（实际上是先再升读上取）\n","#所以恢复的时候，需要先将它们给叠起来，然后再排列行，然后再排列列。\n","#  \"\"\"\n","#这里所说的H实际上对应的是depth，而W对应的width和heigth那一维\n","#局部变量：true_height_sub，true_width_sub是裁剪过后的子图的大小，true_height,true_width是原图的大小\n","\n","def split_image(new_img_list,zoomin=False):\n","    print(new_img_list[0].shape)\n","    split_img_list=[]\n","    height,width=new_img_list[0].shape\n","    true_height_sub,true_width_sub=int(height/split_scale_H),int(width/split_scale_W)\n","    true_height,true_width=height,width\n","    for j in range(0,true_height,true_height_sub):\n","        for k in range(0,true_width,true_width_sub):\n","            for i in range(0,len(new_img_list)-2):\n","                sub_list=[]\n","                for ii in range(i,i+frame_num):\n","                    #在这里需要对img_list进行分割\n","                    img=new_img_list[ii][j:j+true_height_sub,k:k+true_width_sub].copy()\n","                    sub_list.append(img)\n","                split_img_list.append(sub_list)\n","    return split_img_list\n","def toString(i):\n","    if(i<10):\n","        return '000'+str(i)\n","    elif(i<100):\n","        return '00'+str(i)\n","    elif(i<1000):\n","        return '0'+str(i)\n","    else:\n","        return str(i)\n","def remove_dir(dir):\n","    dir = dir.replace('\\\\', '/')\n","    if(os.path.isdir(dir)):\n","        for p in os.listdir(dir):\n","            remove_dir(os.path.join(dir,p))\n","        if(os.path.exists(dir)):\n","            os.rmdir(dir)\n","    else:\n","        if(os.path.exists(dir)):\n","            os.remove(dir)\n","def write_dataset(file_dir_path):\n","    for dir in os.listdir(file_dir_path):\n","        all_dir=file_dir_path+'/'+dir\n","        print(all_dir)\n","        if(os.path.isdir(all_dir)):\n","            continue\n","        new_img_list1,new_img_list2,new_img_list3,new_img_list4=generate_image(all_dir)\n","        split_img_list1=split_image(new_img_list1)\n","        split_img_list2=split_image(new_img_list2)\n","        split_img_list3=split_image(new_img_list3,True)\n","        split_img_list4=split_image(new_img_list4,True)\n","        if(os.path.exists(\"./dataset_new/\"+dir)):\n","            shutil.rmtree(\"./dataset_new/\"+dir)\n","        os.makedirs(\"./dataset_new/\"+dir)\n","        for i in range(len(split_img_list1)):\n","            sub_dir=\"./dataset_new/\"+dir+\"/a\"+toString(i)\n","            os.makedirs(sub_dir)\n","            for j in range(len(split_img_list1[i])):\n","                filename=sub_dir+'/'+toString(j)+'.jpg'\n","                cv2.imwrite(filename,split_img_list1[i][j])\n","        for i in range(len(split_img_list2)):\n","            sub_dir=\"./dataset_new/\"+dir+\"/b\"+toString(i)\n","            os.makedirs(sub_dir)\n","            for j in range(len(split_img_list2[i])):\n","                filename=sub_dir+'/'+toString(j)+'.jpg'\n","                cv2.imwrite(filename,split_img_list2[i][j])\n","        for i in range(len(split_img_list3)):\n","            sub_dir=\"./dataset_new/\"+dir+\"/c\"+toString(i)\n","            os.makedirs(sub_dir)\n","            for j in range(len(split_img_list3[i])):\n","                filename=sub_dir+'/'+toString(j)+'.jpg'\n","                cv2.imwrite(filename,split_img_list3[i][j])\n","        for i in range(len(split_img_list4)):\n","            sub_dir=\"./dataset_new/\"+dir+\"/d\"+toString(i)\n","            os.makedirs(sub_dir)\n","            for j in range(len(split_img_list4[i])):\n","                filename=sub_dir+'/'+toString(j)+'.jpg'\n","                cv2.imwrite(filename,split_img_list4[i][j])\n","#---------------------------------------------------------------------------------------------#\n","#---------------------------------------------------------------------------------------------#\n","\"\"\"\n","1.在这里需要对其进行裁剪,记裁剪比例为\n","2.除了test之外,还应该记录一个新的dataset,用来显示效果,并且它需要排布整齐,大小为模型的倍数\n","display_dataset\n","\n","\"\"\"\n","#---------------------------------------------------------------------------------------------#\n","#---------------------------------------------------------------------------------------------#\n","#这里的all_cnt记录了总共需要读多少组数据，因为数据量太大加载起来费时，所以测试阶段all_cnt==4\n","#再最开始生成src和target的步骤中，先不考虑display，display是再shuffle的过程中才开始考虑的。\n","def dataload(file_dir_path,is_image=False):\n","    dataset_src=[]      #list of tensor[T,1,H,W]\n","    dataset_target=[]    #list of tensor[T,1,H,W]\n","    for dir in os.listdir(file_dir_path):\n","        #在这里首先需要判断是否是文件夹\n","        all_dir=file_dir_path+'/'+dir\n","        if(dir[0]=='.' or (not os.path.isdir(all_dir))):\n","            continue\n","        global all_cnt;\n","        if(all_cnt>=4):#它记录了总共需要读多少个文件\n","            break\n","        all_cnt=all_cnt+1\n","        print(all_dir)\n","        new_img_list1,new_img_list2,new_img_list3,new_img_list4=generate_image(all_dir)\n","        split_img_list1,split_img_list2,split_img_list3,split_img_list4=split_image(new_img_list1),split_image(new_img_list2),split_image(new_img_list3,True),split_image(new_img_list4,True)\n","        for i in range(len(split_img_list1)):\n","            oneVideo=[]\n","            for j in range(len(split_img_list1[i])):\n","                tem=torch.from_numpy(split_img_list1[i][j])\n","                tem=tem.unsqueeze(0)\n","                oneVideo.append(tem)\n","            dataset_src.append(torch.stack(oneVideo))\n","        for i in range(len(split_img_list3)):\n","            oneVideo=[]\n","            for j in range(len(split_img_list3[i])):\n","                tem=torch.from_numpy(split_img_list3[i][j])\n","                tem=tem.unsqueeze(0)\n","                oneVideo.append(tem)\n","            dataset_target.append(torch.stack(oneVideo))\n","    #print(dataset_src)\n","    #在这里需要对dataset_target和dataset_src进行类型转化\n","    #这里需要计算display_dataset的大小\n","    #同时需要注意的是train_dataset中,它是先按一个方向切割,这个方向排布完成后再排布另一个方向\n","    #display_for_one是为了计算一个模型需要多少个video对其进行对应，model_num是一个需要去多少个物体\n","    display_for_one,model_num=split_scale_H*split_scale_W*(find_width-2),1\n","    display_size=display_for_one*model_num\n","    test_dataset_size=display_size\n","    print(len(dataset_target))\n","    train_dataset_src,train_dataset_target=torch.stack(dataset_target[0:-test_dataset_size]),torch.stack(dataset_src[0:-test_dataset_size])\n","    train_dataset_src,train_dataset_target=train_dataset_src.float()/255.0,train_dataset_target.float()/255.0\n","    display_dataset_src,display_dataset_target=torch.stack(dataset_target[-display_size:]),torch.stack(dataset_src[-display_size:])\n","    display_dataset_src,display_dataset_target=display_dataset_src.float()/255.0,display_dataset_target.float()/255.0\n","    print(train_dataset_src.shape)\n","    print(train_dataset_target.shape);\n","    if is_image:\n","        train_dataset_src=torch.flatten(train_dataset_src, start_dim=0, end_dim=1)\n","        train_dataset_target=torch.flatten(train_dataset_target, start_dim=0, end_dim=1)\n","        display_dataset_src=torch.flatten(display_dataset_src, start_dim=0, end_dim=1)\n","        display_dataset_target=torch.flatten(display_dataset_target, start_dim=0, end_dim=1)\n","\n","    train_dataset=TensorDataset(train_dataset_src,train_dataset_target)\n","    display_dataset=TensorDataset(display_dataset_src,display_dataset_target)\n","    test_dataset_src,test_dataset_target=torch.stack(dataset_target[-test_dataset_size:]),torch.stack(dataset_src[-test_dataset_size:])\n","    test_dataset_src,test_dataset_target=test_dataset_src.float()/255.0,test_dataset_target.float()/255.0\n","    print(\"train_dataset:\")\n","    print((train_dataset_src.shape,train_dataset_target.shape))\n","    print(\"display_dataset:\")\n","    print((display_dataset_src.shape,display_dataset_target.shape))\n","    print(\"test_dataset:\")\n","    print((test_dataset_src.shape,test_dataset_target.shape))\n","    if is_image:\n","        test_dataset_src=torch.flatten(test_dataset_src, start_dim=0, end_dim=1)\n","        test_dataset_target=torch.flatten(test_dataset_target, start_dim=0, end_dim=1)\n","    test_dataset=TensorDataset(test_dataset_src,test_dataset_target)\n","    \n","    return train_dataset,test_dataset,display_dataset"],"metadata":{"id":"ayDpiOdjDU4L","executionInfo":{"status":"ok","timestamp":1639048645160,"user_tz":-480,"elapsed":8627,"user":{"displayName":"黎治圻","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15483283087615354405"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["all_cnt=0\n","file_dir_path=\"./data\"\t\t\n","train_dataset,test_dataset,display_dataset=dataload(file_dir_path)\n","train_loader = DataLoader(dataset=train_dataset,\n","                batch_size=train_batch,\n","                shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset,\n","                batch_size=train_batch,\n","                shuffle=True)\n","display_loader=DataLoader(dataset=display_dataset,\n","                batch_size=train_batch,\n","                shuffle=False)\n","#display也是被切割成batch了的,然后后面需要对其进行拼接"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wx-UDv3YNRKl","outputId":"48550351-3fb9-47bf-bb46-77e690aa77d9","executionInfo":{"status":"ok","timestamp":1638986215907,"user_tz":-480,"elapsed":53762,"user":{"displayName":"黎治圻","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15483283087615354405"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["./data/C099\n","('read the model./data/C099,there are files:', 256)\n","(256, 512)\n","(256, 512)\n","(64, 512)\n","(64, 512)\n","./data/C081\n","('read the model./data/C081,there are files:', 256)\n","(256, 512)\n","(256, 512)\n","(64, 512)\n","(64, 512)\n","./data/C050\n","('read the model./data/C050,there are files:', 256)\n","(256, 512)\n","(256, 512)\n","(64, 512)\n","(64, 512)\n","./data/C067\n","('read the model./data/C067,there are files:', 256)\n","(256, 512)\n","(256, 512)\n","(64, 512)\n","(64, 512)\n","65280\n","torch.Size([48960, 3, 1, 16, 64])\n","torch.Size([48960, 3, 1, 64, 64])\n","train_dataset:\n","(torch.Size([48960, 3, 1, 16, 64]), torch.Size([48960, 3, 1, 64, 64]))\n","display_dataset:\n","(torch.Size([16320, 3, 1, 16, 64]), torch.Size([16320, 3, 1, 64, 64]))\n","test_dataset:\n","(torch.Size([16320, 3, 1, 16, 64]), torch.Size([16320, 3, 1, 64, 64]))\n"]}]},{"cell_type":"code","source":["list=[nn.Conv2d(1, 64, kernel_size=3, padding=1, bias=True),]\n","list2=[list[0],]\n","print(id(list[0]))\n","print(id(list2[0]))\n","\"\"\"a=torch.tensor([[[1,2,3],[3,4,5]],[[1,2,3],[3,4,5]]],dtype=float)\n","print(a)\n","print(a.shape)\n","a[:,0]/=2;\n","print(a)\n","print(a.shape)\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"gm4wAJAAjxIT","executionInfo":{"status":"ok","timestamp":1639048775265,"user_tz":-480,"elapsed":380,"user":{"displayName":"黎治圻","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15483283087615354405"}},"outputId":"0e7fd14e-d293-4fd0-9eee-00d0218445dd"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["139852404944592\n","139852404944592\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'a=torch.tensor([[[1,2,3],[3,4,5]],[[1,2,3],[3,4,5]]],dtype=float)\\nprint(a)\\nprint(a.shape)\\na[:,0]/=2;\\nprint(a)\\nprint(a.shape)\\n'"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"cxbmkvnXnjwo"},"source":["1.改造VESPCN模型"]},{"cell_type":"code","metadata":{"id":"8VOaQsQ_eqUC","executionInfo":{"status":"ok","timestamp":1638986215908,"user_tz":-480,"elapsed":19,"user":{"displayName":"黎治圻","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15483283087615354405"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","# TODO: Is the first channel flow with respect to x?\n","# TODO: Fix 'mean' issues\n","\n","\n","class Approx_Huber_Loss(nn.Module):\n","    def __init__(self, args):\n","        super(Approx_Huber_Loss, self).__init__()\n","        self.device = torch.device('cpu' if args.cpu else 'cuda')\n","        self.sobel_filter_X = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]]).reshape((1, 1, 3, 3))\n","        self.sobel_filter_Y = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]).reshape((1, 1, 3, 3))\n","        self.sobel_filter_X = torch.from_numpy(self.sobel_filter_X).float().to(self.device)\n","        self.sobel_filter_Y = torch.from_numpy(self.sobel_filter_Y).float().to(self.device)\n","        self.epsilon = torch.Tensor([0.01]).float().to(self.device)\n","\n","    def forward(self, flow):\n","        flow_X = flow[:, 0:1]\n","        flow_Y = flow[:, 1:]\n","        grad_X = F.conv2d(flow_X, self.sobel_filter_X, bias=None, stride=1, padding=1)\n","        grad_Y = F.conv2d(flow_Y, self.sobel_filter_Y, bias=None, stride=1, padding=1)\n","        huber = torch.sqrt(self.epsilon + torch.sum(grad_X.pow(2)+grad_Y.pow(2)))\n","        return huber"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"kTfTlQMz5PT3","executionInfo":{"status":"ok","timestamp":1638986215909,"user_tz":-480,"elapsed":20,"user":{"displayName":"黎治圻","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15483283087615354405"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","def make_mc():\n","    return MotionCompensator()\n","\n","class MotionCompensator(nn.Module):\n","    def __init__(self):\n","        self.device = 'cuda'\n","        super(MotionCompensator, self).__init__()\n","        print(\"Creating Motion compensator\")\n","        #这里的groups就是有多少个并在一起的\n","        def _gconv(in_channels, out_channels, kernel_size=3, groups=1, stride=1, bias=True):\n","            return nn.Conv2d(in_channels*groups, out_channels*groups, kernel_size, groups=groups, stride=stride,\n","                             padding=(kernel_size // 2), bias=bias)\n","\n","        # Coarse flow\n","        coarse_flow = [_gconv(2, 24, kernel_size=5, groups=1, stride=2), nn.ReLU(inplace=True)]\n","        coarse_flow.extend([_gconv(24, 24, kernel_size=3, groups=1), nn.ReLU(True)])\n","        coarse_flow.extend([_gconv(24, 24, kernel_size=5, groups=1, stride=2), nn.ReLU(True)])\n","        coarse_flow.extend([_gconv(24, 24, kernel_size=3, groups=1), nn.ReLU(True)])\n","        coarse_flow.extend([_gconv(24, 32, kernel_size=3, groups=1), nn.Tanh()])\n","        coarse_flow.extend([nn.PixelShuffle(4)])\n","\n","        self.C_flow = nn.Sequential(*coarse_flow)\n","\n","        # Fine flow\n","        fine_flow = [_gconv(5, 24, kernel_size=5, groups=1, stride=2), nn.ReLU(inplace=True)]\n","        for _ in range(3):\n","            fine_flow.extend([_gconv(24, 24, kernel_size=3, groups=1), nn.ReLU(True)])\n","        fine_flow.extend([_gconv(24, 8, kernel_size=3, groups=1), nn.Tanh()])\n","        fine_flow.extend([nn.PixelShuffle(2)])\n","\n","        self.F_flow = nn.Sequential(*fine_flow)\n","\n","    def forward(self, frame_1, frame_2):\n","        #frame 的大小为 [N, n_colors, H, W]\n","\n","        # Create identity flow\n","        #print(frame_1.shape)\n","        x = np.linspace(-1, 1, frame_1.shape[3]) # x=W\n","        y = np.linspace(-1, 1, frame_1.shape[2]) # y=H\n","        xv, yv = np.meshgrid(x, y)\n","        #(-1,0,1)\n","        #(-1,0,1)\n","        # 3,3,2 也就是说每个格子中存放的是坐标\n","        #xv: [[-1 0 1]\n","        #     [-1 0 1]\n","        #     [-1 0 1]]\n","        #yv: [[1   1  1]\n","        #     [0   0  0]\n","        #     [-1 -1 -1]]\n","        #\n","        #\n","        #\n","        #stack:https://blog.csdn.net/wgx571859177/article/details/80987459\n","        id_flow = np.expand_dims(np.stack([xv, yv], axis=-1), axis=0) #（1，W，H，2）\n","        self.identity_flow = torch.from_numpy(id_flow).float().to(self.device)\n","\n","        # Coarse flow，它输出的Coarse flow已经变成了原来的四倍了\n","        coarse_in = torch.cat((frame_1, frame_2), dim=1)\n","        coarse_out = self.C_flow(coarse_in)\n","        coarse_out[:,0] /= frame_1.shape[3] #也就说这里，这里第二维及以后的都要除frame_1.shape[3]\n","        coarse_out[:,1] /= frame_2.shape[2] #但是为什么要这么做？\n","        frame_2_compensated_coarse = self.warp(frame_2, coarse_out)\n","        \n","        # Fine flow\n","        fine_in = torch.cat((frame_1, frame_2, frame_2_compensated_coarse, coarse_out), dim=1)\n","        fine_out = self.F_flow(fine_in)\n","        fine_out[:,0] /= frame_1.shape[3]\n","        fine_out[:,1] /= frame_2.shape[2]\n","        flow = (coarse_out + fine_out)\n","\n","        frame_2_compensated = self.warp(frame_2, flow)\n","\n","        return frame_2_compensated, flow\n","\n","    def warp(self, img, flow):\n","        # https://discuss.pytorch.org/t/solved-how-to-do-the-interpolating-of-optical-flow/5019\n","        # permute flow N C H W -> N H W C\n","        # clamp把结果加紧到（-1，1）中\n","        #\n","        img_compensated = F.grid_sample(img, (-flow.permute(0,2,3,1)+self.identity_flow).clamp(-1,1), padding_mode='border')\n","        return img_compensated\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQWpfk3wsgIs","executionInfo":{"status":"ok","timestamp":1638986215909,"user_tz":-480,"elapsed":19,"user":{"displayName":"黎治圻","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15483283087615354405"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","#args 中包括2个内容：scale与n_sequence\n","def make_espcn():\n","    return ESPCN_multiframe2()\n","#这个类的forward函数待会还要改一下\n","#fmap的大小是[B,sizeO=scale,H,W]，这里价格H给连起来了\n","#最后得到的是[B,1,H,W]，这里没有什么问题，因为ESPCN中虽然输出的n_sequence=3，但是输出仍然只有一张\n","class myPixelShuffle(nn.Module):\n","    def __init__(self,scale):\n","        super(myPixelShuffle, self).__init__()\n","        self.scale=scale\n","    def forward(self,fmap):\n","        list=[]\n","        for i in range(fmap.shape[2]):\n","            for j in range(fmap.shape[1]):\n","                list.append(fmap[:,j:j+1,i:i+1,:])\n","        return torch.cat(list,dim=2)    \n","def mypixel_shuffle(fmap,scale):\n","\t#这里fmap的维度是:[B,sizeO=scale,H,W],我需要做的是把他们H给连接起来\n","    list=[]\n","    for i in range(fmap.shape[2]):\n","        for j in range(fmap.shape[1]):\n","            list.append(fmap[:,j:j+1,i:i+1,:])\n","    return torch.cat(list,dim=2)\n","#下面的64和差别没有关系，后面可以考虑将其变小，\n","class ESPCN_multiframe2(nn.Module):\n","    # Add Residual connection!\n","    def __init__(self, ):\n","        super(ESPCN_multiframe2, self).__init__()\n","        print(\"Creating ESPCN multiframe2 (x%d)\" % scale)\n","        network = [nn.Conv2d( 3, 64, kernel_size=3, padding=1), nn.ReLU(True)]\n","        network.extend([nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(True)])\n","        network.extend([nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(True)])\n","        network.extend([nn.Conv2d(64, 32, kernel_size=3, padding=1), nn.ReLU(True)])\n","        network.extend([nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(True)])\n","        network.extend([nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(True)])\n","        network.extend([nn.Conv2d(32, 20, kernel_size=3, padding=1), nn.ReLU(True)])\n","        network.extend(\n","            [nn.Conv2d(20, scale, kernel_size=3, padding=1), nn.ReLU(True)])\n","        network.extend([myPixelShuffle(scale)])\n","        network.extend([nn.Conv2d(1, 1, kernel_size=1, padding=0)])\n","        self.net = nn.Sequential(*network)\n","    def forward(self, x):\n","        #if isinstance(x, list):\n","            #通过阅读下面的代码可以发现，其接受到的数据实际上为[N, n_sequence * n_colors, H, W]=[N,n_colors, H, W]\n","            #在我们的模型中n_colors=1\n","            # squeeze frames n_sequence * [N, 1, n_colors, H, W] -> n_sequence * [N, n_colors, H, W]\n","            #lr_frames_squeezed = [torch.squeeze(frame, dim = 1) for frame in x]\n","            # concatenate frames n_sequence * [N, n_colors, H, W] -> [N, n_sequence * n_colors, H, W]\n","            #x = torch.cat(lr_frames_squeezed, dim = 1)    \n","        return self.net(x)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["这里是那里提到了的两个损失函数"],"metadata":{"id":"556jd6w2rR8o"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","# TODO: Is the first channel flow with respect to x?\n","# TODO: Fix 'mean' issues\n","\n","\n","class Approx_Huber_Loss(nn.Module):\n","    def __init__(self):\n","        super(Approx_Huber_Loss, self).__init__()\n","        self.device = torch.device('cuda')\n","        self.sobel_filter_X = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]]).reshape((1, 1, 3, 3))\n","        self.sobel_filter_Y = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]).reshape((1, 1, 3, 3))\n","        self.sobel_filter_X = torch.from_numpy(self.sobel_filter_X).float().to(self.device)\n","        self.sobel_filter_Y = torch.from_numpy(self.sobel_filter_Y).float().to(self.device)\n","        self.epsilon = torch.Tensor([0.01]).float().to(self.device)\n","\n","    def forward(self, flow):\n","        flow_X = flow[:, 0:1]\n","        flow_Y = flow[:, 1:]\n","        grad_X = F.conv2d(flow_X, self.sobel_filter_X, bias=None, stride=1, padding=1)\n","        grad_Y = F.conv2d(flow_Y, self.sobel_filter_Y, bias=None, stride=1, padding=1)\n","        huber = torch.sqrt(self.epsilon + torch.sum(grad_X.pow(2)+grad_Y.pow(2)))\n","        return huber"],"metadata":{"id":"iZNw7LRwrRfy","executionInfo":{"status":"ok","timestamp":1638986215909,"user_tz":-480,"elapsed":20,"user":{"displayName":"黎治圻","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15483283087615354405"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"l1YeKLNk4JYF","executionInfo":{"status":"ok","timestamp":1638986215910,"user_tz":-480,"elapsed":20,"user":{"displayName":"黎治圻","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15483283087615354405"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","#from model.motioncompensator import make_model as make_mc\n","#from model.espcn_multiframe2 import make_model as make_espcn\n","#from approx_huber_loss import Approx_Huber_Loss\n","\n","def make_model(args):\n","    return VESPCN(args)\n","\n","class VESPCN(nn.Module):\n","    def __init__(self):\n","        self.name = 'VESPCN'\n","        self.device = 'cuda'\n","        super(VESPCN, self).__init__()\n","        print(\"Creating VESPCN\")\n","        \n","        self.mseloss = nn.MSELoss()\n","        self.huberloss = Approx_Huber_Loss()\n","        self.motionCompensator = make_mc()\n","        self.espcn = make_espcn()\n","\n","    def forward(self, frame_list):\n","        #frame list中的数据的大小为【N，3，n_colors，H，W】\n","        # 一个sequence中只有3张图，其中第二张图是目标图。n_sequence=3\n","        # squeeze frames n_sequence * [N, 1, n_colors, H, W] -> n_sequence * [N, n_colors, H, W]\n","        #frame_list = [torch.squeeze(frame, dim = 1) for frame in frame_list]\n","        frame1 = torch.squeeze(frame_list[:,0:1,:,:,:],dim=1)\n","        frame2 = torch.squeeze(frame_list[:,1:2,:,:,:],dim=1)\n","        frame3 = torch.squeeze(frame_list[:,2:3,:,:,:],dim=1)\n","        #这里输入的数据的大小为【N，n_colors，H，W】,输出的数据大小为\n","        frame1_compensated, flow1 = self.motionCompensator(frame2, frame1)\n","        frame3_compensated, flow2 = self.motionCompensator(frame2, frame3)\n","        \n","        loss_mc_mse = self.mseloss(frame1_compensated, frame2) + self.mseloss(frame3_compensated, frame2)\n","        loss_mc_huber = self.huberloss(flow1) + self.huberloss(flow2)\n","        \n","        #print(frame1_compensated.shape, frame2.shape, frame3_compensated.shape)\n","        # n_sequence * [N, n_colors, H, W] -> [N, n_sequence * n_colors, H, W]\n","        lr_frames_cat = torch.cat((frame1_compensated, frame2, frame3_compensated), dim = 1) \n","        #print(lr_frames_cat.shape)\n","        #这里说明了espcn中接受到的数据实际上为[N, n_sequence * n_colors, H, W]\n","        return self.espcn(lr_frames_cat), loss_mc_mse, loss_mc_huber\n","\n","class CTTRCN(nn.Module):\n","    def __init__(self,args):\n","        self.name = 'CTTRCN'\n","        self.device = 'cuda'\n","        if args.cpu:\n","            self.device = 'cpu' \n","        super(CTTRCN, self).__init__()\n","        print(\"Creating CTTRCN\")   \n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["编写它的训练器"],"metadata":{"id":"VmwNUbM8shef"}},{"cell_type":"code","source":["#训练程序需要好好改一下\n","#下面开始进行训练\n","#首先需要把X给分成\n","eps=1e-6\n","import torch.nn.functional as func\n","\n","class LossFunc(nn.Module):\n","    def __init__(self):\n","        super(LossFunc, self).__init__()\n","\n","        return\n","\n","    def forward(self,X,Y):\n","        #print(X.shape)\n","        #print(Y.shape)\n","        df1=torch.add(torch.flatten(torch.flatten(X, start_dim=0,end_dim=1),start_dim=1),-torch.flatten(torch.flatten(Y, start_dim=0,end_dim=1),start_dim=1))\n","        df1=torch.sqrt(torch.add(torch.sum(df1.mul(df1),dim=1),eps))\n","        loss=torch.mean(df1)\n","        return loss\n","#这里不一样的地方在于:它们的维度是B,C,H,W\n","#然后得到的任意一张图片，它的处理方法和前面的区别不是特别大,它的顺序是，先再深度层上叠加\n","def display_demo(dataloader,model,loss_fn,device,t):\n","    model.eval()\n","    final_prelist1=[]\n","    final_prelist2=[]\n","    for batch, (X, Y) in enumerate(dataloader):\n","        X=X.to(device)\n","        Y=Y.to(device)\n","\n","        pred = model(X)\n","        #print(pred.shape)\n","        final_prelist1.append(pred)\n","        final_prelist2.append(Y)\n","    output_final(final_prelist1,\"new\")\n","    output_final(final_prelist2,\"ground truth\")\n","def output_final(final_prelist,name):\n","    blendB=torch.stack(final_prelist)\n","    blendB=torch.flatten(blendB, start_dim=0,end_dim=1)\n","    #print(blendB.shape)\n","    final_prelist=[]\n","    height=all_depth\n","    width=find_height\n","    true_height_sub=int(height/split_scale_H)\n","    true_width_sub=int(width/split_scale_W)\n","    #这里把一个方块内所有深度的都给提取出来。\n","    #待会还要确保，它生成的过程中，是在深度上提取的。\n","    for i in range(0,blendB.shape[0],find_width):\n","        img=blendB[i:i+find_width,:,:,:];\n","        final_prelist.append(img)\n","    #这一步得到的是若干张截面图\n","    final_img=[]\n","    for i in range(0,find_width):\n","        img= torch.zeros([1,all_depth,find_height])\n","        final_img.append(img)\n","    for i in range(0,find_width):\n","        for j in range(0,split_scale_H):\n","            for k in range(0,split_scale_W):\n","                index=j*split_scale_W+k\n","                final_img[i][0,j*true_height_sub:(j+1)*true_height_sub,k*true_width_sub:(k+1)*true_width_sub]=final_prelist[index][i,0,:,:]   \n","    #现在final_img中完全是截面了\n","    #现在需要把若干张截面转换成slice\n","    final_slice=[]\n","    for i in range(all_depth):\n","        img= torch.zeros([1,find_height,find_width])\n","        final_slice.append(img)\n","    for i in range(all_depth):\n","        for j in range(find_width):    \n","           # print(final_img[j][0,i,:].shape)\n","            #print(final_slice[i][0,:,j].shape)\n","            #print(final_img[j][0,i:i+1,:])\n","            final_slice[i][0,:,j:j+1]=final_img[j][0,i:i+1,:].reshape((find_height,1))\n","            \n","        print(final_slice[i][0,:,:])\n","    #然后在这里需要进行作图\n","    filename=name+\"display_img_\"+str(t)+\"_\";\n","    for i in range(5):\n","        filename_now=filename+str(i)+\".jpg\"\n","        output_slice=final_slice[i][0,:,:].cpu().detach().numpy()\n","        output_slice=output_slice*255\n","        output_slice_out=np.zeros(output_slice.shape,dtype='uint8')\n","        for j in range(output_slice.shape[0]):\n","            for k in range(output_slice.shape[1]):\n","                output_slice_out[j,k]=int(output_slice[j,k])\n","        cv2.imwrite(filename_now,output_slice_out)\n","def output_todrive(filename,index,image):\n","    filename_now=\"./out/\"+filename+str(index)+\".jpg\"\n","    output_slice=image[0,:,:].cpu().detach().numpy()\n","    output_slice=output_slice*255\n","    output_slice_out=np.zeros(output_slice.shape,dtype='uint8')\n","    for j in range(output_slice.shape[0]):\n","        for k in range(output_slice.shape[1]):\n","            output_slice_out[j,k]=int(output_slice[j,k])\n","        cv2.imwrite(filename_now,output_slice_out)\n","def train(dataloader, model, loss_fn, optimizer,device):\n","    size = len(dataloader.dataset)\n","    model.train()\n","    begin=False\n","    for batch, (X, Y) in enumerate(dataloader):\n","        X=X.to(device)\n","        Y=Y.to(device)\n","        #print(X.shape,Y.shape)\n","        pred,_,_ = model(X)\n","        #print(pred.shape)\n","        loss = loss_fn(pred,Y[:,1,:,:,:])\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        loss= loss.item()\n","\n","        if batch%10==0:\n","            output_todrive(\"train_src\",batch,X[0,1,:,:,:])\n","            output_todrive(\"train_target\",batch,Y[0,1,:,:,:])\n","            output_todrive(\"train_new\",batch,pred[0,:,:,:])\n","            print(f\"loss: {loss:>7f}\")\n","def test(dataloader, model, loss_fn,device,t):\n","    size = len(dataloader.dataset)\n","    model.eval()\n","    begin=False\n","    final_slice=[]\n","    for batch, (X, Y) in enumerate(dataloader):\n","        X=X.to(device)\n","        Y=Y.to(device)\n","\n","        pred,_,_ = model(X)\n","        loss = loss_fn(pred,Y[:,1,:,:,:])\n","        loss= loss.item()\n","        if batch%1==0:\n","            print(f\"loss: {loss:>7f}\")\n","        final_slice.append(pred)\n","    filename=\"test\"+\"display_img_\"+str(t)+\"_\";\n","    for i in range(5):\n","        filename_now=filename+str(i)+\".jpg\"\n","        output_slice=final_slice[0][i,0,:,:].cpu().detach().numpy()\n","        output_slice=output_slice*255\n","        output_slice_out=np.zeros(output_slice.shape,dtype='uint8')\n","        for j in range(output_slice.shape[0]):\n","            for k in range(output_slice.shape[1]):\n","                output_slice_out[j,k]=int(output_slice[j,k])\n","        cv2.imwrite(filename_now,output_slice_out)\n","\n","\n","# Assuming that we are on a CUDA machine, this should print a CUDA device:\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","model=VESPCN().to(device)\n","#定义loss_fn\n","loss_fn=nn.MSELoss()\n","#定义optimizer\n","#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n","optimizer=torch.optim.Adam(model.parameters(),lr=1e-3, weight_decay=0)\n","epochs = 100\n","model.load_state_dict(torch.load('./net/modelVssr1_static'), strict=False);\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    #train(train_loader, model, loss_fn,optimizer,device)\n","    print(\"test--------------------------------\\n\")\n","    test(test_loader, model, loss_fn,device,t)\n","    #display_demo(display_loader, model, loss_fn,device,t)\n","print(\"Done!\")"],"metadata":{"id":"DXDuka-btP0D","colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"status":"error","timestamp":1638986216448,"user_tz":-480,"elapsed":557,"user":{"displayName":"黎治圻","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15483283087615354405"}},"outputId":"10492a91-f5c6-469b-d032-283ac1bac5e9"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n","Creating VESPCN\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-a22bbac783a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVESPCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;31m#定义loss_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-d4472dbeddd3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmseloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuberloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mApprox_Huber_Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotionCompensator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mespcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_espcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-bc8cb2e13abc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msobel_filter_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msobel_filter_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msobel_filter_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msobel_filter_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msobel_filter_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msobel_filter_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"]}]},{"cell_type":"markdown","source":["这里保存这个模型，太凭运气了，一会训练的好一会训练的不好"],"metadata":{"id":"-kgeF3oSCnao"}},{"cell_type":"code","source":["# 保存整个网络\n","torch.save(model, \"./net/modelVssr1\") \n","# 保存网络中的参数, 速度快，占空间少\n","torch.save(model.state_dict(),\"./net/modelVssr1_static\")\n","#--------------------------------------------------\n","#针对上面一般的保存方法，加载的方法分别是：\n","#model_dict=torch.load(PATH)\n","#model_dict=model.load_state_dict(torch.load(PATH))\n","#https://zhuanlan.zhihu.com/p/38056115\n","#这里记录一下加载模型的方法，大概就是这样\n"," #self.motionCompensator.load_state_dict(torch.load('./experiment/model_best_mc.pt'), strict=False)\n","   #     self.espcn.load_state_dict(torch.load('./experiment/model_best_espcn.pt'), strict=False)"],"metadata":{"id":"y9QMKYu0CxYw","executionInfo":{"status":"aborted","timestamp":1638986216446,"user_tz":-480,"elapsed":5,"user":{"displayName":"黎治圻","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15483283087615354405"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nH_onTDM0S5f"},"source":["2.查看并修改zero-shot的代码\n","ZSSR关注的不仅仅是它的模型，还有它的训练方法，不过我们这里不一样的是。\n","我们的流程是，首先对ZSSR进行预训练，即对每一个Video进行训练，然后在训练过程中，对每一个Video都运用对应的ZSSR模型参数。然后这里的问题在于shuffle的时候怎么知道对应的Video的标号。\n"]},{"cell_type":"markdown","source":["下面的这一部分是根据一张图生长一个dataset，需要对其进行修改"],"metadata":{"id":"XC6G7-o0hOLv"}},{"cell_type":"code","source":["\"\"\"\"\n","Taken as is from https://github.com/jacobgil/pytorch-zssr/blob/master/source_target_transforms.py\n","with slight edit in the RandomCrop function to work with tensor instead of pil\n","\"\"\"\n","import numpy as np\n","import random\n","from torchvision.transforms import functional as F\n","import numbers\n","import torch\n","\n","class RandomRotationFromSequence(object):\n","    \"\"\"Rotate the image by angle.\n","    Args:\n","        degrees (sequence or float or int): Range of degrees to select from.\n","            If degrees is a number instead of sequence like (min, max), the range of degrees\n","            will be (-degrees, +degrees).\n","        resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n","            An optional resampling filter.\n","            See http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters\n","            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n","        expand (bool, optional): Optional expansion flag.\n","            If true, expands the output to make it large enough to hold the entire rotated image.\n","            If false or omitted, make the output image the same size as the input image.\n","            Note that the expand flag assumes rotation around the center and no translation.\n","        center (2-tuple, optional): Optional center of rotation.\n","            Origin is the upper left corner.\n","            Default is the center of the image.\n","    \"\"\"\n","\n","    def __init__(self, degrees, resample=False, expand=False, center=None):\n","        self.degrees = degrees\n","        self.resample = resample\n","        self.expand = expand\n","        self.center = center\n","\n","    @staticmethod\n","    def get_params(degrees):\n","        \"\"\"Get parameters for ``rotate`` for a random rotation.\n","        Returns:\n","            sequence: params to be passed to ``rotate`` for random rotation.\n","        \"\"\"\n","        angle = np.random.choice(degrees)\n","        return angle\n","\n","    def __call__(self, data):\n","        \"\"\"\n","            img (PIL Image): Image to be rotated.\n","        Returns:\n","            PIL Image: Rotated image.\n","        \"\"\"\n","        hr, lr = data\n","        angle = self.get_params(self.degrees)\n","        return F.rotate(hr, angle, self.resample, self.expand, self.center), \\\n","                F.rotate(lr, angle, self.resample, self.expand, self.center)\n","\n","class RandomHorizontalFlip(object):\n","    \"\"\"Horizontally flip the given PIL Image randomly with a probability of 0.5.\"\"\"\n","\n","    def __call__(self, data):\n","        \"\"\"\n","        Args:\n","            img (PIL Image): Image to be flipped.\n","        Returns:\n","            PIL Image: Randomly flipped image.\n","        \"\"\"\n","        hr, lr = data\n","        if random.random() < 0.5:\n","            return F.hflip(hr), F.hflip(lr)\n","        return hr, lr\n","\n","class RandomVerticalFlip(object):\n","    \"\"\"Vertically flip the given PIL Image randomly with a probability of 0.5.\"\"\"\n","\n","    def __call__(self, data):\n","        \"\"\"\n","        Args:\n","            img (PIL Image): Image to be flipped.\n","        Returns:\n","            PIL Image: Randomly flipped image.\n","        \"\"\"\n","        hr, lr = data\n","        if random.random() < 0.5:\n","            return F.vflip(hr), F.vflip(lr)\n","        return hr, lr\n","\n","class RandomCrop(object):\n","    \"\"\"Crop the given PIL Image at a random location.\n","    Args:\n","        size (sequence or int): Desired output size of the crop. If size is an\n","            int instead of sequence like (h, w), a square crop (size, size) is\n","            made.\n","        padding (int or sequence, optional): Optional padding on each border\n","            of the image. Default is 0, i.e no padding. If a sequence of length\n","            4 is provided, it is used to pad left, top, right, bottom borders\n","            respectively.\n","    \"\"\"\n","\n","    def __init__(self, size, padding=0):\n","        if isinstance(size, numbers.Number):\n","            self.size = (int(size), int(size))\n","        else:\n","            self.size = size\n","        self.padding = padding\n","\n","    @staticmethod\n","    def get_params(data, output_size):\n","        \"\"\"Get parameters for ``crop`` for a random crop.\n","        Args:\n","            img (PIL Image): Image to be cropped.\n","            output_size (tuple): Expected output size of the crop.\n","        Returns:\n","            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n","        \"\"\"\n","        hr, lr = data\n","        w, h = hr.shape[-2], hr.shape[-1]\n","        th, tw = output_size\n","        if w == tw or h == th:\n","            return 0, 0, h, w\n","\n","        if w < tw or h < th:\n","            th, tw = h//2, w//2\n","\n","        i = random.randint(0, h - th)\n","        j = random.randint(0, w - tw)\n","        return i, j, th, tw\n","\n","    def __call__(self, data):\n","        hr, lr = data\n","        i, j, h, w = self.get_params(data, self.size)\n","        lr = lr[:, j:j + w, i:i + h]\n","        hr = hr[:, j:j + w, i:i + h]\n","        return hr, lr\n","\n","class ToTensor(object):\n","    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n","    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n","    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n","    \"\"\"\n","\n","    def __call__(self, data):\n","        \"\"\"\n","        Args:\n","            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n","        Returns:\n","            Tensor: Converted image.\n","        \"\"\"\n","        hr, lr = data\n","        if type(lr) is not torch.Tensor:\n","            lr = F.to_tensor(lr)\n","\n","        if type(hr) is not torch.Tensor:\n","            hr = F.to_tensor(hr)\n","        return hr, lr"],"metadata":{"id":"8oFOwkY_wV3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","\n","from PIL import Image\n","from torch.utils.data import Sampler, Dataset\n","from transforms import *\n","from torchvision.transforms import transforms\n","from utils import show_images\n","\n","class ZSSRDataset(Dataset):\n","    def __init__(self, source_image, sr_factor):\n","        super(ZSSRDataset, self).__init__()\n","        self.sr_factor = sr_factor\n","        self.source_image = source_image\n","        smaller_side = min(self.source_image.size[0: 2])\n","        larger_side = max(self.source_image.size[0: 2])\n","\n","        factors = []\n","        for i in range(smaller_side // 2, smaller_side + 1):\n","            downsampled_smaller_side = i\n","            zoom = float(downsampled_smaller_side) / smaller_side\n","            if downsampled_smaller_side % self.sr_factor == 0 and not（zoom==0）:\n","                factors.append(zoom)\n","        hr_lr_pairs = []\n","        for zoom in factors:\n","            hr = self.source_image.resize((int(self.source_image.size[0] * zoom),\n","                                  int(self.source_image.size[1] * zoom)),\n","                                 resample=Image.BICUBIC)\n","            lr = hr.resize((int(hr.size[0] / self.sr_factor),\n","                            int(hr.size[1])),\n","                           resample=Image.BICUBIC)\n","\n","            hr_lr_pairs.append((hr, lr))\n","        self.hr_lr_pairs = hr_lr_pairs\n","        self.transform = transforms.Compose([\n","            RandomRotationFromSequence([0, 90, 180, 270]),\n","            RandomHorizontalFlip(),\n","            RandomVerticalFlip()])\n","    def __getitem__(self, index):\n","        return self.transform(self.hr_lr_pairs[index])\n","\n","    def __len__(self):\n","        return len(self.hr_lr_pairs)\n","\n","    @classmethod\n","    def from_image(cls, img, sr_factor):\n","        pil = transforms.ToPILImage()(transforms.ToTensor()(img))\n","        return ZSSRDataset(pil, sr_factor)\n","\n","    def show_pairs(self):\n","        number_of_pairs = len(self)\n","        indexes = random.sample(range(number_of_pairs), k=4)\n","        pairs = [self.__getitem__(index) for index in indexes]\n","        lrs = [lr for _, lr in pairs]\n","        hrs = [hr for hr, _ in pairs]\n","        show_images(lrs + hrs, 2, 4)\n","\n","    def concat(self, dataset):\n","        self.hr_lr_pairs += dataset.hr_lr_pairs\n","        return self\n","\n","class ZSSRSampler(Sampler):\n","    def __init__(self, dataset):\n","        super(ZSSRSampler, self).__init__(dataset)\n","        self.dataset = dataset\n","        sizes = np.float32([(hr.size[0] * hr.size[1] / float(\n","            self.dataset.source_image.size[0] * self.dataset.source_image.size[1])) for hr, lr in self.dataset.hr_lr_pairs])\n","        self.pair_probabilities = sizes / np.sum(sizes)\n","\n","    def __iter__(self):\n","        while True:\n","            yield random.choices(self.dataset, weights=self.pair_probabilities, k=1)[0]\n"],"metadata":{"id":"xsM77VM7hMZr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["这里定义的是ZSSR模型，然后和ESPCN模型的思路莫一样，是先插值，然后再使用残差网络进行重建"],"metadata":{"id":"TfubJT0Fquxq"}},{"cell_type":"code","source":["class ZSSRModel(nn.Module):\n","    #在这里的输入应该是要一张图片，然后它的大小和hr图片的大小是一致的，所以在这里没有必要修改它\n","    def __init__(self,device='cuda'):\n","        super(ZSSRModel, self).__init__()\n","        ks=3\n","        layers_num=8\n","        Conv2dList=[nn.Conv2d(1, 64, kernel_size=ks, padding=ks // 2, bias=True),]\n","        layers = [\n","            Conv2dList[0],\n","            nn.ReLU()\n","        ]\n","        for i in range(layers_num - 2):\n","            Conv2dList+=[nn.Conv2d(64, 64, kernel_size=ks, padding=ks//2, bias=True),]\n","            layers += [\n","                Conv2dList[i+1],\n","                nn.ReLU()\n","            ]\n","        Conv2dList+=[nn.Conv2d(64, 1, kernel_size=ks, padding=ks//2, bias=True),]\n","        layers += [Conv2dList[layers_num-1],]\n","        self.model = nn.Sequential(*layers)\n","        self.device = device\n","        self.to(device)\n","\n","    def forward(self, X):\n","      #这里使用的是残差网络\n","        return self.model(X) + X\n","    def freezing(self)：\n","        for o in Conv2dList:\n","            for para in o.parameters():\n","                para.requires_grad = False\n"],"metadata":{"id":"hB_ZLkh85-qU","executionInfo":{"status":"aborted","timestamp":1638986216447,"user_tz":-480,"elapsed":6,"user":{"displayName":"黎治圻","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15483283087615354405"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["这一个是对ZSSR模型的训练函数\n","在这里我仍然按照它的想法进行，因为它的对upsampling过后的结果使用残差网络是有一定的道理的。因为在训练数据不大的时候，尽可能在插值的基础上进行改造可能要合理的多。\n","所以后面我还需要做一步从hr到feature map的过程\n","\n","\n","其中：data_sampler就是通过运行上面的步骤得到的采样器\n","\n","在训练的过程中，它的上采样函数仍然是一个神经网络。，就是ESPCN网络，我"],"metadata":{"id":"yvWpV2a6h16d"}},{"cell_type":"code","source":["def bicubic_upsample(lr, scale):\n","    lr = lr.resize([lr.size[0] * scale, lr.size[1]],\n","                   resample=Image.BICUBIC)\n","    return transforms.ToTensor()(lr)\n","def train_single_img(lr_img,\n","                     model,\n","                    optimizer,\n","                    num_batches,\n","                    device=\"cuda\"):\n","#在调用的时候，这里的num_batches为\n","    dataset = ZSSRDataset(lr_img, scale)\n","    data_sampler = ZSSRSampler(dataset)\n","    model.train()\n","    avg_loss = 0\n","    l1_loss = F.l1_loss\n","    trans = transforms.Compose([\n","        ToTensor(),\n","        RandomCrop(config[\"crop_size\"])\n","    ])\n","    for iter, (hr, lr) in enumerate(data_sampler):\n","        optimizer.zero_grad()\n","        scale = hr.size[0] // lr.size[0]\n","        lr = bicubic_upsample(lr, scale)\n","        hr, lr = trans((hr, lr))#没有必要裁剪。因为它给的那个裁剪的范围有点大，我们的达不到那个范围\n","        #看样子这里一个batch的大小应该是1\n","        hr, lr = hr.unsqueeze(0).to(device), lr.unsqueeze(0).to(device)\n","        hr_pred = model(lr)\n","        loss = l1_loss(hr_pred, hr)\n","\n","        avg_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if iter > num_batches:\n","            print('Done training.')\n","            avg_loss /= iter\n","            print(f'Avg training loss is {avg_loss}')\n","            break"],"metadata":{"id":"TtLjRJHXhN5_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["然后开始写融合的以及还原feature map的部分\n","\n","这里是需要仿照前面的函数写一个逆函数"],"metadata":{"id":"EZYBJTzm4kmx"}},{"cell_type":"code","source":["class myInvPixelShuffle(nn.Module):\n","    def __init__(self,scale):\n","        super(myPixelShuffle, self).__init__()\n","        self.scale=scale\n","    def forward(self,sr_img):\n","      #sr_img的形状是【B，1，r*H，W】，我需要先将其还原为【B，r，H，W】\n","        final_list=[]\n","        for i in range(self.scale):\n","            final_list.append([])\n","        for i in range(fmap.shape[2]):\n","            index=i%self.scale\n","            final_list[index].append(sr_img[:,:,i:i+1,:])\n","        for i in range(self.scale):\n","            torch.cat(final_list[index],dim=2)\n","        return   torch.cat(final_list,dim=1)\n","def myInvpixel_shuffle(fmap,scale):\n","\t#这里fmap的维度是:[B,sizeO=scale,H,W],我需要做的是把他们H给连接起来\n","    final_list=[]\n","    for i in range(scale):\n","        final_list.append([])\n","    for i in range(fmap.shape[2]):\n","        index=i%scale\n","        final_list[index].append(sr_img[:,:,i:i+1,:])\n","    for i in range(scale):\n","        torch.cat(final_list[index],dim=2)\n","    return   torch.cat(final_list,dim=1)"],"metadata":{"id":"CRkoNa4m5JqA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["这一个类就是融合他们的类"],"metadata":{"id":"Y5Oyk59oAa1l"}},{"cell_type":"code","source":["class BlendModel(nn.Module):\n","    #在这里的输入应该是要一张图片，然后它的大小和hr图片的大小是一致的，所以在这里没有必要修改它\n","    def __init__(self,device='cuda'):\n","        super(BlendModel, self).__init__()\n","        ks=3\n","        layers_num=8\n","        layers = [\n","            nn.Conv2d(4*scale, 64, kernel_size=ks, padding=ks // 2, bias=True),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=ks, padding=ks // 2, bias=True),\n","            nn.ReLU(),\n","            nn.Conv2d(64, scale, kernel_size=ks, padding=ks // 2, bias=True),\n","            nn.ReLU(),\n","            myPixelShuffle(scale)\n","        ]\n","        self.model = nn.Sequential(*layers)\n","        self.device = device\n","        self.to(device)\n","\n","    def forward(self, X):\n","      #这里使用的是残差网络\n","        return self.model(X)"],"metadata":{"id":"DLGdxBJLAYpD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["这里亟需解决的问题是，在生成dataset过后，因为会shuffle我应该怎么去获取他们的序号"],"metadata":{"id":"QU7VEIDUAMke"}}]}